# üöÄ GUIDE D√âMARRAGE RAPIDE - ENVIRONNEMENT IA QRADAR

## üìã PR√âREQUIS POUR DEMAIN

### 1. **Nouvelle VM Vagrant avec plus de RAM**
```bash
# Dans le Vagrantfile, augmenter la RAM √† 8GB minimum
config.vm.provider "virtualbox" do |vb|
  vb.memory = "8192"  # 8GB RAM
  vb.cpus = 4         # 4 CPU cores
end
```

### 2. **D√©marrer la nouvelle VM**
```bash
vagrant up
vagrant ssh
```

## üéØ D√âMARRAGE AUTOMATIQUE

### Option 1: Script complet (recommand√©)
```bash
cd /vagrant/Docker
chmod +x setup_ollama_complete.sh
./setup_ollama_complete.sh
```

### Option 2: Script IA puissante (si 8GB+ RAM)
```bash
cd /vagrant/Docker
chmod +x setup_powerful_ai.sh
./setup_powerful_ai.sh
```

## üîß CONFIGURATION MANUELLE (si n√©cessaire)

### 1. **V√©rifier l'environnement**
```bash
# V√©rifier Docker
docker --version
docker-compose --version

# V√©rifier les fichiers
ls -la docker-compose-ollama.yml
ls -la retriever/app.py
```

### 2. **D√©marrer les services**
```bash
# D√©marrer tout
docker-compose -f docker-compose-ollama.yml up -d

# V√©rifier les services
docker ps
```

### 3. **Installer le mod√®le IA**
```bash
# Pour TinyLlama (l√©ger)
curl -X POST http://localhost:11434/api/pull -d '{"name": "tinyllama:1.1b-chat"}'

# Pour Mistral 7B (puissant - 8GB+ RAM)
curl -X POST http://localhost:11434/api/pull -d '{"name": "mistral:7b-instruct-q4_K_M"}'
```

### 4. **Tester l'installation**
```bash
# Test Ollama
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "tinyllama:1.1b-chat", "prompt": "Bonjour", "stream": false}'

# Test Retriever
curl http://localhost:5001/health

# Test Analyse
curl -X POST http://localhost:5001/analyze \
  -H "Content-Type: application/json" \
  -d '{"payload":"Test QRadar"}'
```

## üåê ACC√àS AUX SERVICES

| Service | URL | Port | Description |
|---------|-----|------|-------------|
| **Interface Web** | http://localhost:5000 | 5000 | Interface utilisateur |
| **API Retriever** | http://localhost:5001 | 5001 | API d'analyse IA |
| **Health Check** | http://localhost:5001/health | 5001 | √âtat des services |
| **Statistiques** | http://localhost:5001/stats | 5001 | Stats du syst√®me |
| **Ollama** | http://localhost:11434 | 11434 | Serveur IA local |
| **ChromaDB** | http://localhost:8000 | 8000 | Base vectorielle |
| **MySQL** | localhost:3306 | 3306 | Base de donn√©es |

## üß† MOD√àLES IA DISPONIBLES

### Mod√®les l√©gers (2-4GB RAM)
- **TinyLlama 1.1B** : `tinyllama:1.1b-chat` (1GB RAM)
- **Phi-3 Mini** : `phi3:mini` (2GB RAM)

### Mod√®les puissants (8GB+ RAM)
- **Mistral 7B** : `mistral:7b-instruct-q4_K_M` (4GB RAM)
- **Llama2 7B** : `llama2:7b-chat-q4_K_M` (4GB RAM)

## üìù COMMANDES UTILES

### Gestion des services
```bash
# Voir les logs
docker logs mistral_retriever
docker logs qradar_ticket

# Red√©marrer un service
docker-compose -f docker-compose-ollama.yml restart retriever

# Arr√™ter tout
docker-compose -f docker-compose-ollama.yml down

# Voir les statistiques
curl http://localhost:5001/stats
```

### Gestion des mod√®les IA
```bash
# Lister les mod√®les install√©s
curl http://localhost:11434/api/tags

# Supprimer un mod√®le
curl -X DELETE http://localhost:11434/api/delete -d '{"name": "model_name"}'

# Installer un nouveau mod√®le
curl -X POST http://localhost:11434/api/pull -d '{"name": "model_name"}'
```

### Tests et diagnostics
```bash
# Test de sant√© complet
curl http://localhost:5001/health | jq .

# Test d'analyse simple
curl -X POST http://localhost:5001/analyze \
  -H "Content-Type: application/json" \
  -d '{"payload":"Test payload QRadar"}'

# Test direct Ollama
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "tinyllama:1.1b-chat", "prompt": "Test", "stream": false}'
```

## üîç D√âPANNAGE

### Probl√®mes courants

#### 1. **Service ne d√©marre pas**
```bash
# V√©rifier les logs
docker logs mistral_retriever --tail 50

# Reconstruire l'image
docker-compose -f docker-compose-ollama.yml build retriever
```

#### 2. **Erreur de m√©moire**
```bash
# V√©rifier la RAM disponible
free -h

# Utiliser un mod√®le plus l√©ger
curl -X POST http://localhost:11434/api/pull -d '{"name": "tinyllama:1.1b-chat"}'
```

#### 3. **Erreur MySQL**
```bash
# V√©rifier la base de donn√©es
docker exec -it mysql_payload mysql -u root -proot -e "SHOW DATABASES;"
```

#### 4. **Erreur ChromaDB**
```bash
# Red√©marrer ChromaDB
docker-compose -f docker-compose-ollama.yml restart chromadb
```

## üéØ FONCTIONNALIT√âS

### ‚úÖ Ce qui fonctionne
- **IA locale** : TinyLlama/Mistral via Ollama
- **RAG** : Recherche d'analyses similaires
- **Embeddings** : Stockage vectoriel ChromaDB
- **Base de donn√©es** : MySQL + SQLite
- **Interface web** : Analyse de payloads QRadar
- **API REST** : Endpoints d'analyse
- **Logs complets** : Tra√ßabilit√© des analyses

### üîÑ Apprentissage automatique
- **Contexte historique** : Utilise les analyses pr√©c√©dentes
- **Similarit√©** : Trouve des cas similaires
- **Am√©lioration** : Le syst√®me s'am√©liore avec l'usage

## üöÄ OPTIMISATIONS POUR DEMAIN

### Avec plus de RAM (8GB+)
1. **Mistral 7B** : Meilleure qualit√© d'analyse
2. **Plus de contexte** : Analyses plus d√©taill√©es
3. **Parall√©lisation** : Traitement plus rapide

### Am√©liorations possibles
1. **GPU** : Acc√©l√©ration mat√©rielle
2. **Mod√®les sp√©cialis√©s** : IA d√©di√©e cybers√©curit√©
3. **Interface avanc√©e** : Dashboard temps r√©el

## üìû SUPPORT

En cas de probl√®me :
1. V√©rifier les logs : `docker logs mistral_retriever`
2. Tester les services : `curl http://localhost:5001/health`
3. Red√©marrer : `docker-compose -f docker-compose-ollama.yml restart`

---

**üéâ Votre environnement d'IA locale pour QRadar est pr√™t !** 