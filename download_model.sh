#!/bin/bash

# Script de tÃ©lÃ©chargement du modÃ¨le Mistral 7B
# Utilisation: HF_HUB_TOKEN=<votre_token> bash download_model.sh

set -e

echo "ğŸš€ TÃ‰LÃ‰CHARGEMENT DU MODÃˆLE MISTRAL 7B"
echo "======================================"

# VÃ©rifier le token Hugging Face
if [ -z "$HF_HUB_TOKEN" ]; then
    echo "âŒ Erreur: HF_HUB_TOKEN non dÃ©fini"
    echo "Usage: HF_HUB_TOKEN=<votre_token> bash download_model.sh"
    exit 1
fi

# CrÃ©er les dossiers
mkdir -p Docker/models/mistral-7b
mkdir -p Docker/cache

echo "ğŸ“ Dossiers crÃ©Ã©s"

# TÃ©lÃ©charger le modÃ¨le
echo "ğŸ“¥ TÃ©lÃ©chargement du modÃ¨le Mistral 7B..."
echo "â³ Cela peut prendre plusieurs minutes..."

python3 - <<EOF
import os
from transformers import AutoTokenizer, AutoModelForCausalLM

# Configuration
os.environ['HF_HUB_TOKEN'] = '${HF_HUB_TOKEN}'
model_path = "Docker/models/mistral-7b"

print("ğŸ”§ TÃ©lÃ©chargement du tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    "mistralai/Mistral-7B-v0.1", 
    trust_remote_code=True,
    cache_dir="Docker/cache"
)
tokenizer.save_pretrained(model_path)
print("âœ… Tokenizer tÃ©lÃ©chargÃ©")

print("ğŸ”§ TÃ©lÃ©chargement du modÃ¨le...")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    trust_remote_code=True,
    torch_dtype="auto",
    device_map="auto",
    cache_dir="Docker/cache"
)
model.save_pretrained(model_path)
print("âœ… ModÃ¨le tÃ©lÃ©chargÃ©")

print("ğŸ‰ TÃ©lÃ©chargement terminÃ© avec succÃ¨s!")
EOF

echo ""
echo "âœ… ModÃ¨le Mistral 7B tÃ©lÃ©chargÃ© dans Docker/models/mistral-7b"
echo "ğŸ“Š Taille du modÃ¨le: $(du -sh Docker/models/mistral-7b | cut -f1)"
echo ""
echo "ğŸš€ Vous pouvez maintenant lancer:"
echo "   cd Docker && docker-compose up -d" 